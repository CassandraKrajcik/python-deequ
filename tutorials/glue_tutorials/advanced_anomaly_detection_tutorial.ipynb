{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using AWS Glue and PyDeequ to detect anomalies in large datasets\n",
    "\n",
    "PyDeequ allows us to persist the metrics we computed on dataframes in a so-called MetricsRepository using AWS Glue. In the following example, we showcase how to store a metrics file in S3, query them later on to detect anomalous data. Afterwards, when an anomaly is detected an SNS notification is sent.\n",
    "\n",
    "This tutorial is part of a PyDeequ Glue post. This tutorial uses a dataset queried from AWS Athena."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>107</td><td>application_1595892420059_0108</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-32-2-231.us-west-2.compute.internal:20888/proxy/application_1595892420059_0108/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-32-17-239.us-west-2.compute.internal:8042/node/containerlogs/container_1595892420059_0108_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "from awsglue.utils import getResolvedOptions \n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "\n",
    "\n",
    "glueContext = GlueContext(SparkContext.getOrCreate())\n",
    "session = glueContext.spark_session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the Data table \n",
    "Let us demo the Holt Winters Anomaly Strategy on the `jewelry_dataset`, the newly created table from AWS Athena.  \n",
    "\n",
    "To use PyDeequ on `jewelry_dyf` convert the dataset to a dataframe using `.toDF()`. Next, use the `dropDuplicates` method to remove any potential duplicates within the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+-----------+\n",
      "|total_votes|year|review_date|\n",
      "+-----------+----+-----------+\n",
      "|          7|2014|  2014-4-01|\n",
      "|          7|2014|  2014-4-01|\n",
      "|          7|2014|  2014-4-01|\n",
      "|          7|2014|  2014-4-01|\n",
      "|          7|2014|  2014-4-01|\n",
      "+-----------+----+-----------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "jewelry_dyf = glueContext.create_dynamic_frame.from_catalog(database=\"jewelry_db\", table_name=\"jewelry_dataset\")\n",
    "\n",
    "jewelry_df = jewelry_dyf.toDF()\n",
    "jewelry_df.dropDuplicates()\n",
    "\n",
    "jewelry_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform the Data table\n",
    "\n",
    "It looks like the `jewelry_df` can be further simplified. Using the `date_format` method, we can change the column to only show the month and year of the total_votes. Afterwards, we can `filter` the `jewelry_df2` by year and contain only the two needed columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pyspark.sql.functions as f\n",
    "\n",
    "jewelry_df2 = jewelry_df.withColumn('review_date', f.date_format('review_date', 'yyyy/M'))\\\n",
    "        .orderBy('review_date', ascending = False)\n",
    "\n",
    "\n",
    "df_2013 = jewelry_df2.filter(\"year ='2013'\").select(\"review_date\",\"total_votes\")\n",
    "df_2014 = jewelry_df2.filter(\"year ='2014'\").select(\"review_date\",\"total_votes\")\n",
    "df_2015 = jewelry_df2.filter(\"year ='2015'\").select(\"review_date\",\"total_votes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|review_date|total_votes|\n",
      "+-----------+-----------+\n",
      "|     2013/9|          2|\n",
      "|     2013/9|          2|\n",
      "|     2013/9|          2|\n",
      "|     2013/9|          8|\n",
      "|     2013/9|          2|\n",
      "|     2013/9|          8|\n",
      "|     2013/9|          2|\n",
      "|     2013/9|          2|\n",
      "|     2013/9|          2|\n",
      "|     2013/9|          8|\n",
      "+-----------+-----------+\n",
      "only showing top 10 rows"
     ]
    }
   ],
   "source": [
    "df_2013.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let us use PyDeequ to detect anomalous data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Metrics Repository\n",
    "\n",
    "We will be demoing with the `FileSystemMetricsRepository` class. A metrics repository can be used as a data quality report overtime. \n",
    "\n",
    "**Metrics Repository allows us to store the metrics in json format on S3.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# S3 write path \n",
    "s3_write_path = \"s3://devpydeequ/tmp/holt_winters_tutorial.json\"\n",
    "\n",
    "import pydeequ\n",
    "from pydeequ.repository import *\n",
    "\n",
    "metricsRepository = FileSystemMetricsRepository(session,s3_write_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We will be running a Holt Winters Anomaly Strategy on our current datatasets. \n",
    "\n",
    "Holt Winters checks for trends, seasonality and average within a dataset. Therefore, to create these averages Deequ uses two cycles of data to forecast datapoints. \n",
    "\n",
    "Our dataset today is collected monthly, and follows an annual seasonal trend. Let us use the `MetricInterval.Monthly` and `SeriesSeasonality.Yearly`. This selection requires us to collect at least 25 datapoints. The initial 24 datapoints are monthly values from 2013 and 2014, which will be used to create the Holt Winters model. The following values in 2015 are the forecasted points, which could can concede an anomalous value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will be loading the 2013 dataset into a metrics file "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let us create a `for` loop that will be iterating through `df_2013`. Using `month` we can create a `date` to later help us query values from `df_2013`. The `filter` method allows us create a `df` dataframe which contains the `total_votes` values by month (e.x. the first iteration will be a table of values from January 2013). \n",
    "\n",
    "\n",
    "Next each set of metrics that we computed needs be indexed by a so-called `ResultKey`, which contains a timestamp and supports arbitrary tags in the form of key-value pairs.\n",
    "\n",
    "Finally, we create a `VerificationSuite`. We make Deequ write and store our metrics in S3 by adding the `useRepository` and the `saveOrAppendResult` method. Then we add the `Holt Winters` with a `Sum` analyzer to calculate monthly `total_votes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pydeequ.verification import *\n",
    "\n",
    "for month in range(1,13):\n",
    "    date = \"\\'2013/\"+str(month)+\"\\'\"\n",
    "    df = df_2013.filter(\"review_date =\" + date)\n",
    "    \n",
    "    key_tags = {'tag':  date}\n",
    "    result_key_2013 = ResultKey(session, ResultKey.current_milli_time(), key_tags)\n",
    "\n",
    "    jewelry_result = VerificationSuite(session).onData(df)\\\n",
    "        .useRepository(metricsRepository) \\\n",
    "        .saveOrAppendResult(result_key_2013) \\\n",
    "        .addAnomalyCheck(HoltWinters(session, MetricInterval.Monthly, SeriesSeasonality.Yearly), Sum('total_votes'))\\\n",
    "        .run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will be creating an anomaly check for 2014\n",
    "\n",
    "This process is very similar to loading the 2013 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for month in range(1,13):\n",
    "    date = \"\\'2014\" +'/'+str(month)+\"\\'\"\n",
    "    df = df_2014.filter(\"review_date =\" + date)\n",
    "    key_tags = {'tag':  date}\n",
    "    result_key_2014 = ResultKey(session, ResultKey.current_milli_time(), key_tags)\n",
    "\n",
    "    jewelry_result = VerificationSuite(session).onData(df)\\\n",
    "        .useRepository(metricsRepository) \\\n",
    "        .saveOrAppendResult(result_key_2014) \\\n",
    "        .addAnomalyCheck(HoltWinters(session, MetricInterval.Monthly, SeriesSeasonality.Yearly), Sum('total_votes'))\\\n",
    "        .run()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let us load the current metrics repository. \n",
    "The repository now has the monthly total votes for 2013 to 2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+----+-------+-------------+---------+\n",
      "|entity|   instance|name|  value| dataset_date|      tag|\n",
      "+------+-----------+----+-------+-------------+---------+\n",
      "|Column|total_votes| Sum|41199.0|1597783116602| '2013/1'|\n",
      "|Column|total_votes| Sum|32570.0|1597783122280| '2013/2'|\n",
      "|Column|total_votes| Sum|31366.0|1597783125923| '2013/3'|\n",
      "|Column|total_votes| Sum|28408.0|1597783129935| '2013/4'|\n",
      "|Column|total_votes| Sum|28395.0|1597783133643| '2013/5'|\n",
      "|Column|total_votes| Sum|25896.0|1597783137074| '2013/6'|\n",
      "|Column|total_votes| Sum|27524.0|1597783140544| '2013/7'|\n",
      "|Column|total_votes| Sum|29689.0|1597783144126| '2013/8'|\n",
      "|Column|total_votes| Sum|25246.0|1597783147704| '2013/9'|\n",
      "|Column|total_votes| Sum|27364.0|1597783151084|'2013/10'|\n",
      "|Column|total_votes| Sum|25710.0|1597783154598|'2013/11'|\n",
      "|Column|total_votes| Sum|50431.0|1597783158101|'2013/12'|\n",
      "|Column|total_votes| Sum|42801.0|1597783165759| '2014/1'|\n",
      "|Column|total_votes| Sum|41202.0|1597783169756| '2014/2'|\n",
      "|Column|total_votes| Sum|35845.0|1597783173508| '2014/3'|\n",
      "|Column|total_votes| Sum|29683.0|1597783177344| '2014/4'|\n",
      "|Column|total_votes| Sum|29519.0|1597783181148| '2014/5'|\n",
      "|Column|total_votes| Sum|26354.0|1597783184970| '2014/6'|\n",
      "|Column|total_votes| Sum|31237.0|1597783188704| '2014/7'|\n",
      "|Column|total_votes| Sum|27434.0|1597783192445| '2014/8'|\n",
      "+------+-----------+----+-------+-------------+---------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "analysisResult_metRep = metricsRepository.load() \\\n",
    "                            .before(ResultKey.current_milli_time()) \\\n",
    "                            .getSuccessMetricsAsDataFrame()\n",
    "\n",
    "analysisResult_metRep.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Great!  We have created the trend for the Holt Winters algorithm. \n",
    "Now let us detect any anomalies within 2015. If you want to recieve an e-mail notification create a variable `topicArns` containing the arn of the SNS topic you created and an `snsClient` with your region. \n",
    "\n",
    "Next create another HoltWinters anomaly check similar to the 2013 and 2014 dataset. Except iterate from month 1-8 (the dataset only went up to August of 2015). \n",
    "\n",
    "Within the for loop check for an anomaly using the `jewelry_result.status`. If it is not a success, that means an anomaly has been detected. Next, `collect` the `constraint_message` to see the error value. \n",
    "\n",
    "Now let us create an SNS Notification! \n",
    "Use `publish` to create an SNS notification. Include the `topicArn`, a `Message`, `subject` and `MessageAttribute`. If an anomaly has been detected break out of the loop. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c248ba568fd493eb4b09cc9f9a3837e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use AWS SNS \n",
    "import boto3 \n",
    "import json\n",
    "\n",
    "# Topic for AWS SNS \n",
    "topicArn = 'arn:aws:sns:us-west-2:498730894712:jewelry_hw'\n",
    "snsClient = boto3.client('sns', region_name = 'us-west-2')\n",
    "\n",
    "\n",
    "for month in range(1,9):\n",
    "    date = \"\\'2015\" +'/'+str(month)+\"\\'\"\n",
    "    df = df_2015.filter(\"review_date =\" + date)\n",
    "    key_tags = {'tag':  date}\n",
    "    result_key_2015 = ResultKey(session, ResultKey.current_milli_time(), key_tags)\n",
    "\n",
    "    jewelry_result = VerificationSuite(session).onData(df)\\\n",
    "        .useRepository(metricsRepository) \\\n",
    "        .saveOrAppendResult(result_key_2015) \\\n",
    "        .addAnomalyCheck(HoltWinters(session, MetricInterval.Monthly, SeriesSeasonality.Yearly), Sum('total_votes'))\\\n",
    "        .run()\n",
    "    \n",
    "    df = VerificationResult.checkResultsAsDataFrame(session, jewelry_result)\n",
    "    \n",
    "    if (jewelry_result.status != \"Success\"):\n",
    "        print(\"Anomaly for total_votes has been detected\")\n",
    "        print(date)\n",
    "        message = df.select(\"constraint_message\").collect()\n",
    "        response = snsClient.publish(TopicArn = topicArn,\n",
    "                             Message = \"anomaly detected in data frame: \\n\" + json.dumps(message),\n",
    "                             Subject = \"Anomaly Detected in the jewelry dataset:\"+ date,\n",
    "                             MessageAttributes = {\"TransactionType\":\n",
    "                                            {\"DataType\": \"String.Array\", \"StringValue\": \"Anomaly Detected in Glue\"}})\n",
    "        break\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recieved an e-mail finding an anomaly for February 2015. Looking back at our graphed dataset this supports our hypothesis of having a Holt Winters Anomaly on February 2015. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}